{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7885ab5957ec1d3a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "version = \"2.1.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2e41d1ba65fad0d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='toc'></a>  \n",
    "# Table of Contents\n",
    "- **[Assignment Description](#Topic0)**\n",
    "- **[Topic 1 - Text Processing](#Topic1)**\n",
    "  - [Task 1](#t1)\n",
    "- **[Topic 2 - Latent Semantic Indexing](#Topic2)**\n",
    "  - [Task 2a](#t2a)\n",
    "  - [Task 2b](#t2b)\n",
    "- **[Topic 3 - Semantic Similarity](#Topic3)**\n",
    "  - [Task 3](#t3_)\n",
    "  - [Task 3a](#t3a)\n",
    "  - [Task 3b](#t3b)\n",
    "  - [Task 3c](#t3c) \n",
    "- **[Topic 4 - Topic Coherence](#Topic4)**\n",
    "  - [Task 4a](#t4a)\n",
    "  - [Task 4b](#t4b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either of the following is no longer\n",
    "# necessary for matplotlib in notebooks.\n",
    "# The import statement below has you covered!\n",
    "\n",
    "# %matplotlib notebook\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-64b4374420d1d161",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='Topic0'></a>\n",
    "# SIADS 543 Assignment 3:\n",
    "## Text representations, topic modeling and word embeddings\n",
    "\n",
    "In this week's assignment you'll gain experience applying topic modeling and other latent variable estimation methods. We'll focus on textual data, continuing to work with vectorizers and related text representations like embeddings.\n",
    "\n",
    "All questions in this assignment are auto-graded. Some parts ask you a short question or two about on the results: these are meant to encourage you to reflect on the outcomes, but do not need to be included as part of your graded submission.  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress all warnings only when absolutely necessary\n",
    "# Warnings are in place for a reason!\n",
    "import warnings\n",
    "\n",
    "# warnings.filterwarnings('ignore')\n",
    "# warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e8d8014231a6151c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# First import some necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "up, down = True, False\n",
    "pd.set_option('display.max_rows', 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "\n",
    "## Additional imports can be inlcuded here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8bf7ebbd04db03d7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### The following cell contains a couple of helpful utility functions for use with this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-172dd4cf129d7dcd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# display_topics:  example showing how to take the model components generated by LDA or NMF\n",
    "# and use them to dump the top words by weight for each topic.\n",
    "\n",
    "\n",
    "def display_topics(model, feature_names, num_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\n",
    "            \" \".join(\n",
    "                [feature_names[i] for i in topic.argsort()[: -num_top_words - 1 : -1]]\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "# load_newsgroup_documents: prepare training and test data from the 20newsgroups dataset\n",
    "def load_newsgroup_documents():\n",
    "    # The Coursera environment must be self-contained and so APIs that do external fetching\n",
    "    # aren't allowed. So we use pickle files that can be stored locally instead of the following\n",
    "    # API calls.\n",
    "    # dataset_train   = fetch_20newsgroups(subset = 'train', shuffle=True, random_state=42, remove=('headers', 'footers', 'quotes'))\n",
    "    # dataset_test    = fetch_20newsgroups(subset = 'test', shuffle=True, random_state=42, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "    pickle_train_data = open(\"../../voc/public/assets/20newsgroups_train_data.pickle\", \"rb\")\n",
    "    pickle_train_labels = open(\"../../voc/public/assets/20newsgroups_train_labels.pickle\", \"rb\")\n",
    "    documents_train = pickle.load(pickle_train_data)\n",
    "    labels_train = pickle.load(pickle_train_labels)\n",
    "    pickle_train_data.close()\n",
    "    pickle_train_labels.close()\n",
    "\n",
    "    return documents_train, labels_train\n",
    "\n",
    "\n",
    "# load the dataset for future use....\n",
    "documents_train, labels_train = load_newsgroup_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dffc4f4e38036a59",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='Topic1'></a>\n",
    "## Topic 1 - Text Processing (20 points).\n",
    "### The choice of text processing can impact final classification performance.\n",
    "\n",
    "There are many different parameter settings for Vectorizer objects in scikit-learn. Small changes in these settings can result in very different text representations and significant changes in final classifier accuracy. For this Task you'll train a commonly-used type of text classifier, Multinomial Naive Bayes, using three different input representations for text, to see the effect of different parameter choices on classifier training set accuracy.  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8ceaa45190716940",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='t1'></a>\n",
    "## Task 1 Create and train multiple vectorizers (20 points).\n",
    "Follow these steps:\n",
    "1. Create a TfidfVectorizer object (let's call it A) with the following settings:\n",
    "\n",
    "    `max_features = 10000, # only top 10k by freq`\n",
    "    \n",
    "    `lowercase = False, # keep capitalization`\n",
    "    \n",
    "    `ngram_range = (1,2), # include 2-word phrases`\n",
    "    \n",
    "    `min_df=10,  # note: absolute count of documents`\n",
    "    \n",
    "    `max_df=0.95,   # note: % of docs in collection`\n",
    "    \n",
    "    `stop_words='english'`\n",
    "    \n",
    "    \n",
    "2. Create a CountVectorizer object (let's call it B) with the same settings:\n",
    "\n",
    "    `max_features = 10000, # only top 10k by freq`\n",
    "    \n",
    "    `lowercase = False, # keep capitalization`\n",
    "    \n",
    "    `ngram_range = (1,2), # include 2-word phrases`\n",
    "    \n",
    "    `min_df=10,  # note: absolute count of doc`\n",
    "    \n",
    "    `max_df=0.95,   # note: % of docs`\n",
    "    \n",
    "    `stop_words='english'`\n",
    "    \n",
    "    \n",
    "3. Create a TfidfVectorizer object (let's call it C) with the settings:\n",
    "\n",
    "    `max_features = 10000, # only top 10k by freq`\n",
    "    \n",
    "    `lowercase = False, `\n",
    "    \n",
    "    `ngram_range = (1,2), `\n",
    "    \n",
    "    `min_df=200,  # note: absolute count of docs`\n",
    "    \n",
    "    `max_df=0.95  # note: % of docs` \n",
    "    \n",
    "    \n",
    "4. Using the training data `documents_train`, along with the ground truth labels `labels_train`, train three Naive Bayes classifiers, corresponding to choices A, B, and C of vectorizer.\n",
    "\n",
    "5. Normally we'd compute the accuracy of these classifiers on a test set, but for this question we're interested more in the potential upper bound on performance that is achievable with text representation choices A, B, or C.  Thus you should compute, for each of the three classifiers, the accuracy on the *training set*.\n",
    "\n",
    "6. Your function should return these three accuracy scores as a tuple with three floats: (accuracy_A, accuracy_B, accuracy_C).\n",
    "\n",
    "It is instructive to examine the difference in accuracy across the three different representations.  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ce0aaf11d3f59591",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8352046680222792, 0.7694279904517726, 0.5562726549376713)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_classifiers(documents_train, labels_train):\n",
    "    # Create vectorizer A\n",
    "    A = TfidfVectorizer(max_features=10000,\n",
    "                        lowercase=False,\n",
    "                        ngram_range=(1,2),\n",
    "                        min_df=10,\n",
    "                        max_df=0.95,\n",
    "                        stop_words='english')\n",
    "    \n",
    "    # Create vectorizer B\n",
    "    B = CountVectorizer(max_features=10000,\n",
    "                        lowercase=False,\n",
    "                        ngram_range=(1,2),\n",
    "                        min_df=10,\n",
    "                        max_df=0.95,\n",
    "                        stop_words='english')\n",
    "    \n",
    "    # Create vectorizer C\n",
    "    C = TfidfVectorizer(max_features=10000,\n",
    "                        lowercase=False,\n",
    "                        ngram_range=(1,2),\n",
    "                        min_df=200,\n",
    "                        max_df=0.95)\n",
    "    \n",
    "    # Transform training data\n",
    "    X_train_A = A.fit_transform(documents_train)\n",
    "    X_train_B = B.fit_transform(documents_train)\n",
    "    X_train_C = C.fit_transform(documents_train)\n",
    "    \n",
    "    # Train Naive Bayes classifiers\n",
    "    clf_A = MultinomialNB().fit(X_train_A, labels_train)\n",
    "    clf_B = MultinomialNB().fit(X_train_B, labels_train)\n",
    "    clf_C = MultinomialNB().fit(X_train_C, labels_train)\n",
    "    \n",
    "    # Compute accuracy on training set\n",
    "    accuracy_A = accuracy_score(labels_train, clf_A.predict(X_train_A))\n",
    "    accuracy_B = accuracy_score(labels_train, clf_B.predict(X_train_B))\n",
    "    accuracy_C = accuracy_score(labels_train, clf_C.predict(X_train_C))\n",
    "    \n",
    "    return (accuracy_A, accuracy_B, accuracy_C)\n",
    "def answer_text_processing():\n",
    "    return train_classifiers(documents_train, labels_train)\n",
    "\n",
    "answer_text_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell to explore your solution\n",
    "# remember to comment the function call before submitting the notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "1-graded",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 - AG tests\n",
      "Task 1 - your answer: (0.8352046680222792, 0.7694279904517726, 0.5562726549376713)\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "task_id = \"1\"\n",
    "print(f\"Task {task_id} - AG tests\")\n",
    "stu_ans = answer_text_processing()\n",
    "\n",
    "print(f\"Task {task_id} - your answer: {stu_ans}\")\n",
    "\n",
    "assert isinstance(stu_ans, tuple), \"Task 1: Your function should return a tuple.\"\n",
    "assert len(stu_ans) == 3, \"Task 1: Your tuple should contain three floats.\"\n",
    "\n",
    "for i, item in enumerate(stu_ans):\n",
    "    assert isinstance(\n",
    "        item, (float, np.floating)\n",
    "    ), f\"Task 1: Your answer at index {i} should be a float number. \"\n",
    "\n",
    "# Some hidden tests\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3d0505dd6f24836c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='Topic2'></a>\n",
    "## Topic 2 - Latent Semantic Indexing and the vocabulary gap (30 points).\n",
    "\n",
    "One of the original motivations for Latent Semantic Indexing was overcoming the `vocabulary gap` in information retrieval.\n",
    "A query like `economic budget` should match strongly against text like `government spending on the economy` even though they don't have any exact keywords in common.\n",
    "\n",
    "In this question we'll create a demonstration of the power of Latent Semantic Indexing to do semantic matching. In the first part, you'll run LSI and use the reduced document matrix to do semantic matching of a query against other text that has no terms explicitly in common.\n",
    "\n",
    "In the second part, you'll see how this semantic matching is happening by computing the related terms that are included a query expanded using LSI's latent topics.  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0d79b1fd48df5fe5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='t2a'></a>\n",
    "### Task 2a Use the reduced document matrix from LSI to do semantic matching of a query against a document (15 points).\n",
    "\n",
    "As a first step, run the code below that we've provided that creates a tf.idf vectorizer and applies it to the 20newsgroups training set. It also runs LSI (in reality a TruncatedSVD) with a latent space of 200 dimensions.\n",
    "\n",
    "Suppose we have a query \"economic budget\" that has the tf.idf vector $q$, with shape 1 x num_terms. We can obtain this vector simply by using vectorizer.transform on the text. Think of the matrix $U_k$ as the super operator that converts from original term space to latent semantic space. To expand text $q$ with related terms according to LSI, compute the expanded query $q_k$ using the formula \n",
    "\n",
    "$q_k = q \\cdot (U_k\\Sigma^{-1}_k)$\n",
    "\n",
    "With this formula, you'll \"expand\" both the query and the document vectors to add related terms, and then compute the similarity match between them.\n",
    "\n",
    "Let's walk through these steps (**Note** that in matrix math dimensions and hence order of operations matters!).  \n",
    "1. The _reduced term matrix_, $U_k$, is _multiplied_ (* operator) with the inverse of matrix $\\Sigma_k$, the LSI.singular_values_ matrix. **Note** that $\\Sigma_k$ is raised to the power of negative one $\\Sigma^{-1}_k$. The reason we invert $\\Sigma_k$ is that there is no division operation with matrices so we invert and multiply!\n",
    " - Think of this step as forming a normalized scaler for the LSI latent factor weights (the 'topics').  \n",
    "\n",
    "\n",
    "2. The vectorized query matrix $q$ (or document $d$) is then dotted (dot-product) with the result of $U_k \\Sigma^{-1}_k$.\n",
    "\n",
    "For this question, use cosine similarity to compute the similarity match between any two pieces of text, no matter what their vector representation.\n",
    "\n",
    "With the formula above, consider the query `\"economic budget\"` being matched against the (very) short document `\"government spending on the economy\"`.\n",
    "\n",
    "Your function should return a tuple of two floats: the cosine similarity score (from sklearn.metrics.pairwise) of (a) the original query and document vectors and (b) the LSI-expanded query and document vectors using the method above.\n",
    "\n",
    "Did LSI help overcome the vocabulary gap?  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.linalg import diagsvd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f7fad0f295241fbc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Run this preamble code to run LSI. We've also given the line of code that gets the resulting U matrix.\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 1), min_df=10, max_df=0.95, stop_words=\"english\", max_features=10000\n",
    ")  # default English stopwords\n",
    "\n",
    "tfidf_documents = tfidf_vectorizer.fit_transform(documents_train)\n",
    "\n",
    "# .get_feature_names() is deprecated in 1.0\n",
    "# tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# LSI does truncated SVD on the document-term matrix of tf.idf term-weights.\n",
    "# The matrix we got back from the vectorizer is a\n",
    "# document-term matrix, i.e. one row per document.\n",
    "n_topics = 200\n",
    "lsi = TruncatedSVD(n_components=n_topics, random_state=0)\n",
    "\n",
    "# To match the examples and development of LSI in\n",
    "# our lectures, we're going to\n",
    "# take the transpose of the document-term matrix to give\n",
    "# TruncatedSVD the term-document matrix as input.\n",
    "\n",
    "# This is the matrix U_k:  num_term_features x num_topics\n",
    "reduced_term_matrix = lsi.fit_transform(np.transpose(tfidf_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-03e5c2867559c41b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def answer_semantic_similarity_a():\n",
    "    # original vectors\n",
    "    q = tfidf_vectorizer.transform([\"economic budget\"])\n",
    "    d = tfidf_vectorizer.transform([\"government spending on the economy\"])\n",
    "\n",
    "    # Expand the query with related terms according to LSI\n",
    "    Uk = reduced_term_matrix \n",
    "    Sk = lsi.singular_values_\n",
    "\n",
    "    qk = q.dot(Uk.dot(np.diag(1/Sk)))\n",
    "    dk = (d.dot(Uk)).dot(np.diag(1/Sk))\n",
    "\n",
    "    # Compute the cosine similarity between the query and document vectors\n",
    "    similarity_original = cosine_similarity(q, d)\n",
    "    similarity_lsi = cosine_similarity(qk, dk)\n",
    "\n",
    "    # Return the cosine similarity scores of the original and LSI-expanded vectors\n",
    "    result = (similarity_original[0][0], similarity_lsi[0][0])\n",
    "    # print(result)\n",
    "\n",
    "    return result\n",
    "# answer_semantic_similarity_a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "2a-graded",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2a - AG tests\n",
      "Task 2a - your answer: (0.0, 0.38756039137547116)\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "task_id = \"2a\"\n",
    "print(f\"Task {task_id} - AG tests\")\n",
    "\n",
    "stu_ans = answer_semantic_similarity_a()\n",
    "print(f\"Task {task_id} - your answer: {stu_ans}\")\n",
    "\n",
    "assert isinstance(stu_ans, tuple), \"Task 2a: Your function should return a tuple. \"\n",
    "assert len(stu_ans) == 2, \"Task 2a: Your tuple should contain two floats.\"\n",
    "\n",
    "for i, item in enumerate(stu_ans):\n",
    "    assert isinstance(\n",
    "        item, (float, np.floating)\n",
    "    ), f\"Task 2a: Your answer at index {i} should be a float number. \"\n",
    "\n",
    "# Some hidden tests\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7dcb7722fb75a664",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='t2b'></a>\n",
    "## Task 2b - More understanding of Semantic Matching (15 points).\n",
    "### Which terms does LSI find similar?\n",
    "\n",
    "To understand why the LSI-expanded vectors get the results they do, we're going to look at what the operator $U$ does to text. In particular, the term-term matrix $UU^T$ tells us the term expansion behavior of this LSI model. Think of the term-term matrix like an operator that first maps a term to the latent space $L_k$ (using $U$), then back again from $L_k$ to term space (using $U$ transpose). The $(i,j)$ entry of $UU^T$ is a kind of *association weight* between term $i$ and term $j$.\n",
    "\n",
    "Write a function to get the most related terms (according to LSI) for the word \"economy\". To do this:\n",
    "\n",
    "1. Compute the term-term matrix from the matrix U  (the reduced_term_matrix variable).\n",
    "2. Use the term-term matrix to get the association weights of all words related to the term \"economy\"\n",
    "3. Sort by descending weight value.\n",
    "4. Your function should return the top 5 words and their weights as a list of (string, float) tuples.\n",
    "\n",
    "Do the related terms match your subjective similarity judgment?  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-980257c3a923230b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def answer_semantic_similarity_b():\n",
    "    t = \"economy\"\n",
    "    top = 5\n",
    "    term_term_matrix = np.dot(reduced_term_matrix, np.transpose(reduced_term_matrix))\n",
    "    term_index = tfidf_vectorizer.vocabulary_[t]\n",
    "    # print(\"top lsi-related terms for\", t, \":\")\n",
    "    top_related_terms_indexes = term_term_matrix[term_index, :].argsort()[::-1]\n",
    "    # print(top_related_terms_indexes[:5])\n",
    "    topSet = []\n",
    "    for i in range(0, top):\n",
    "        this_term = top_related_terms_indexes[i]\n",
    "        # print('\\t{} ({:.2f})'.format(tfidf_feature_names[this_term],term_term_matrix[term_index, this_term]))\n",
    "        topSet.append((tfidf_feature_names[this_term], term_term_matrix[term_index, this_term]))\n",
    "    topSet.sort(key=lambda x: x[1], reverse=True)\n",
    "    # print(topSet)\n",
    "\n",
    "    return topSet\n",
    "# answer_semantic_similarity_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "2b-graded",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2b - AG tests\n",
      "Task 2b - your answer:\n",
      "[('government', 0.418059810620016), ('people', 0.25474921519912636), ('money', 0.2071115007881263), ('clinton', 0.1972790419264127), ('tax', 0.17710173696845163)]\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "task_id = \"2b\"\n",
    "print(f\"Task {task_id} - AG tests\")\n",
    "\n",
    "stu_ans = answer_semantic_similarity_b()\n",
    "print(f\"Task {task_id} - your answer:\\n{stu_ans}\")\n",
    "\n",
    "assert isinstance(stu_ans, list), \"Task 2b: Your function should return a list. \"\n",
    "assert (\n",
    "    len(stu_ans) == 5\n",
    "), \"Task 2b: Your list should contain five elements (the term, score tuples).\"\n",
    "\n",
    "for i, item in enumerate(stu_ans):\n",
    "    assert isinstance(\n",
    "        item, tuple\n",
    "    ), f\"Task 2b: Your answer at index {i} should be a tuple. \"\n",
    "    assert isinstance(\n",
    "        item[0], str\n",
    "    ), f\"Task 2b: The first element of your tuple at index {i} should be a string. \"\n",
    "    assert isinstance(\n",
    "        item[1], (float, np.floating)\n",
    "    ), f\"Task 2b: The second element of your tuple at index {i} should be a float. \"\n",
    "\n",
    "# Some hidden tests\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e5ef4ed71becdbc8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='Topic3'></a>\n",
    "## Topic 3 - Semantic Similarity (20 points total):\n",
    "**Comparing your ranking with word2vec's ranking.**  \n",
    "For this task we will be comparing your word similarity determination to that of a pre-trained Word2Vec transformer model, Text8.\n",
    "\n",
    "### Before proceeding:\n",
    " - The following code cell must be executed to load the pre-trained word2vec model.\n",
    " - The result is an instance of the class W2VTransformer(size=100, min_count=1, seed=2)\n",
    "   - `from gensim.sklearn_api import W2VTransformer`\n",
    " - For reading the binary file into memory, we now use the Python context handler `with` rather than `open`/`close`. This is considered to be more 'Pythonic.'  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.sklearn_api import W2VTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-95b5dd8117445a6c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"../../voc/public/assets/text8_W2V.pickle\", \"rb\") as fh:\n",
    "    text8_model = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c84217cfd11646f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='t3_'></a>\n",
    "### Task 3 - Ranking words by yourself and then generating word2vec's ranking\n",
    "\n",
    "The first part of the task is to rearrange the terms in the `my_ranking` tuple based on your subjective impression of how similar they are to the term `\"party\"`. Encode your impression with terms arranged in **decreasing** order of similarity to the term `\"party\"` e.g., index 0 holds `\"party\"`, index 1 the next most similar term all the way to index 8 holding the term you consider the least similar. **This is your personal subjective understanding of how similar these terms are to the word** `\"party\"`. **Your** arrangement is the correct answer for this task! \n",
    "\n",
    "For example, if you believe the term `\"event\"` is the most similar `\"party\"`, it should be placed in second position within the tuple and so on. Make sure that your final tuple contain all nine terms, and beginning with `\"party\"` (index 0).\n",
    "\n",
    "_ranking tuple original ordering_  \n",
    "my_ranking = (\"party\", \"bicycle\", \"vote\", \"lead\", \"election\", \"champagne\", \"event\", \"fun\", \"budget\",)  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THE FOLLOWING variable my_ranking.\n",
    "\n",
    "my_ranking = (\"party\",\"lead\",\"vote\",\"fun\",\"event\",\"election\",\"champagne\",\"budget\",\"bicycle\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5e54c46f1e77b7a5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# system reference\n",
    "\n",
    "reference_terms = (\n",
    "    \"party\",\n",
    "    \"bicycle\",\n",
    "    \"vote\",\n",
    "    \"lead\",\n",
    "    \"election\",\n",
    "    \"champagne\",\n",
    "    \"event\",\n",
    "    \"fun\",\n",
    "    \"budget\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c09bc882b90ad2cd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='t3a'></a>\n",
    "### Task 3a - Extract the Text8 model's expression of similarity between \"party\" and the remaining terms.\n",
    "\n",
    "#### One way to do this:\n",
    "1. Use the word2vec `text8_model` to convert `my_ranking` into the corresponding set of word embedding vectors.\n",
    "2. Determine the cosine similarity of each word embedding relative to the target embedding of `\"party\"`. Maintain a list of tuples in the format of `(similarity_score, word)`. Do not skip over the embedding for the target word `\"party\"`, the list should include the similarity to itself.\n",
    "3. Sort the list in decreasing order. Remember, when sorting elements of type tuple, the first element determines the sorting order and the second element is only considered if there is a tie.\n",
    "4. Create a list variable called `system_ranking` which contains only the second element of each tuple (the words). \n",
    "\n",
    "Your function should return a tuple in the format of `(my_ranking, system_ranking)`.  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.base import TransformerMixin\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('party', 'lead', 'vote', 'fun', 'event', 'election', 'champagne', 'budget', 'bicycle'), ['party', 'election', 'vote', 'budget', 'event', 'lead', 'champagne', 'bicycle', 'fun'])\n"
     ]
    }
   ],
   "source": [
    "wordvecs = text8_model.transform(my_ranking)   #makes nd.array \n",
    "tw_embed = wordvecs[0]\n",
    "analysis = []\n",
    "for i in range(0, len(my_ranking)):\n",
    "    # print(\"for \",my_ranking[i], wordvecs[i])\n",
    "    similarity = 1 - cosine(tw_embed, wordvecs[i])\n",
    "    analysis.append((similarity,my_ranking[i]))\n",
    "# Sort the list in decreasing order by the first element\n",
    "analysis.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# Extract a list of strings containing only the second element of each tuple\n",
    "system_ranking = [word for score, word in analysis]\n",
    "result=(my_ranking, system_ranking)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rank_words(my_ranking):\n",
    "#     wordvecs = text8_model.transform(my_ranking)   #makes nd.array \n",
    "\n",
    "#     # Determine the cosine similarity of each word embedding relative to the target embedding of \"party\"\n",
    "#     similarity_scores = []\n",
    "#     target_embedding = text8_model['party']\n",
    "#     for i, word in enumerate(my_ranking):\n",
    "#         similarity = np.dot(word_embeddings[i], target_embedding) / (np.linalg.norm(word_embeddings[i]) * np.linalg.norm(target_embedding))\n",
    "#         similarity_scores.append((similarity, word))\n",
    "    \n",
    "#     # Sort the list in decreasing order\n",
    "#     similarity_scores.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "#     # Create a list variable called system_ranking which contains only the second element of each tuple (the words)\n",
    "#     system_ranking = [x[1] for x in similarity_scores]\n",
    "    \n",
    "#     return (my_ranking, system_ranking)\n",
    "# rank_words(my_ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0c0ab801540e5261",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def answer_word2vec_a():\n",
    "\n",
    "    wordvecs = text8_model.transform(my_ranking)   #makes nd.array \n",
    "    tw_embed = wordvecs[0]\n",
    "    analysis = []\n",
    "    for i in range(0, len(my_ranking)):\n",
    "        # print(\"for \",my_ranking[i], wordvecs[i])\n",
    "        similarity = 1 - cosine(tw_embed, wordvecs[i])\n",
    "        analysis.append((similarity,my_ranking[i]))\n",
    "    # Sort the list in decreasing order by the first element\n",
    "    analysis.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    # Extract a list of strings containing only the second element of each tuple\n",
    "    system_ranking = tuple([word for score, word in analysis])\n",
    "    result=(my_ranking, system_ranking)\n",
    "    return result\n",
    "# answer_word2vec_a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell to explore your solution\n",
    "# remember to comment the function call before submitting the notebook\n",
    "\n",
    "# answer_word2vec_a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "3a-graded",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3a - AG tests\n",
      "Task 3a - your answer: (('party', 'lead', 'vote', 'fun', 'event', 'election', 'champagne', 'budget', 'bicycle'), ('party', 'election', 'vote', 'budget', 'event', 'lead', 'champagne', 'bicycle', 'fun'))\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "task_id = \"3a\"\n",
    "print(f\"Task {task_id} - AG tests\")\n",
    "stu_ans = answer_word2vec_a()\n",
    "\n",
    "print(f\"Task {task_id} - your answer: {stu_ans}\")\n",
    "\n",
    "assert isinstance(stu_ans, tuple), \"Task 3a: Your function should return a tuple. \"\n",
    "\n",
    "assert (\n",
    "    len(stu_ans) == 2\n",
    "), \"Task 3a: Your tuple should contain two elements: a tuple of strings (my_ranking), and a tuple of strings (system_ranking).\"\n",
    "\n",
    "# check my_rankings\n",
    "assert isinstance(\n",
    "    stu_ans[0], tuple\n",
    "), \"Task 3a: Your first element must be a tuple (of strings). \"\n",
    "\n",
    "assert len(stu_ans[0]) == len(\n",
    "    reference_terms\n",
    "), \"Task 3a: Your my_rankings tuple doesn't have the expected number of terms.\"\n",
    "\n",
    "assert (\n",
    "    stu_ans[0][0] == reference_terms[0]\n",
    "), \"Task 3a: Your my_rankings tuple must have 'party' as the first term.\"\n",
    "\n",
    "# must be a permutation of the official term set\n",
    "assert set(stu_ans[0]) == set(\n",
    "    reference_terms\n",
    "), \"Task 3a: Your my_rankings tuple is not a permutation of the permitted terms.\"\n",
    "\n",
    "# check system_rankings\n",
    "assert isinstance(\n",
    "    stu_ans[1], tuple\n",
    "), \"Task 3a: Your second element must be a tuple (of strings). \"\n",
    "\n",
    "assert len(stu_ans[1]) == len(\n",
    "    reference_terms\n",
    "), \"Task 3a: Your system_rankings tuple doesn't have the expected number of terms.\"\n",
    "\n",
    "assert (\n",
    "    stu_ans[0][0] == reference_terms[0]\n",
    "), \"Task 3a: Your system_rankings tuple must have 'party' as the first term.\"\n",
    "\n",
    "# must be a permutation of the official term set\n",
    "assert set(stu_ans[1]) == set(\n",
    "    reference_terms\n",
    "), \"Task 3a: Your system_rankings tuple is not a permutation of the permitted terms.\"\n",
    "\n",
    "# Some hidden tests\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6ef0782037a74bf8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='t3b'></a>\n",
    "### Task 3b Comparing rankings with Spearman correlation coefficient\n",
    "\n",
    "Given the rankings you generated above in the format of `(my_ranking, system_ranking)`, use the [`scipy.stats.spearmanr`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html) function which will return a SciPy object containing the spearman correlation and the p-value of your ranking compared to the system ranking, as a measurement of how well they agree. Convert this spearman result to a tuple, and append that tuple to your previous results, i.e. it should have a format of `(my_ranking, system_ranking, spearman_tuple)`.  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d3cd084f9be90457",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('party',\n",
       "  'lead',\n",
       "  'vote',\n",
       "  'fun',\n",
       "  'event',\n",
       "  'election',\n",
       "  'champagne',\n",
       "  'budget',\n",
       "  'bicycle'),\n",
       " ('party',\n",
       "  'election',\n",
       "  'vote',\n",
       "  'budget',\n",
       "  'event',\n",
       "  'lead',\n",
       "  'champagne',\n",
       "  'bicycle',\n",
       "  'fun'),\n",
       " (0.5, 0.17047066078705375))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_word2vec_b():\n",
    "    my_ranking, system_ranking =answer_word2vec_a()\n",
    "    spearman_result = stats.spearmanr(my_ranking, system_ranking)\n",
    "    spearman_tuple = tuple(spearman_result)\n",
    "\n",
    "    results = (my_ranking, system_ranking, spearman_tuple)\n",
    "\n",
    "\n",
    "    return results\n",
    "answer_word2vec_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell to explore your solution\n",
    "# remember to comment the function call before submitting the notebook\n",
    "\n",
    "# answer_word2vec_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "3b-graded",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3b - AG tests\n",
      "Task 3b - your answer: (('party', 'lead', 'vote', 'fun', 'event', 'election', 'champagne', 'budget', 'bicycle'), ('party', 'election', 'vote', 'budget', 'event', 'lead', 'champagne', 'bicycle', 'fun'), (0.5, 0.17047066078705375))\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "task_id = \"3b\"\n",
    "print(f\"Task {task_id} - AG tests\")\n",
    "stu_ans = answer_word2vec_b()\n",
    "\n",
    "print(f\"Task {task_id} - your answer: {stu_ans}\")\n",
    "\n",
    "assert isinstance(stu_ans, tuple), \"Task 3a: Your function should return a tuple. \"\n",
    "assert (\n",
    "    len(stu_ans) == 3\n",
    "), \"Task 3a: Your tuple should contain three elements: a tuple of strings (my_ranking), a tuple of strings (system_ranking), a tuple with spearman output (2 floats).\"\n",
    "\n",
    "# check my_rankings\n",
    "assert isinstance(\n",
    "    stu_ans[0], tuple\n",
    "), \"Task 3a: Your first element must be a tuple (of strings). \"\n",
    "assert len(stu_ans[0]) == len(\n",
    "    reference_terms\n",
    "), \"Task 3a: Your my_rankings tuple doesn't have the expected number of terms.\"\n",
    "assert (\n",
    "    stu_ans[0][0] == reference_terms[0]\n",
    "), \"Task 3a: Your my_rankings tuple must have 'party' as the first term.\"\n",
    "assert set(stu_ans[0]) == set(\n",
    "    reference_terms\n",
    "), \"Task 3a: Your my_rankings tuple is not a permutation of the permitted terms.\"  # must be a permutation of the official term set\n",
    "\n",
    "# check system_rankings\n",
    "assert isinstance(\n",
    "    stu_ans[1], tuple\n",
    "), \"Task 3a: Your second element must be a tuple (of strings). \"\n",
    "assert len(stu_ans[1]) == len(\n",
    "    reference_terms\n",
    "), \"Task 3a: Your system_rankings tuple doesn't have the expected number of terms.\"\n",
    "assert (\n",
    "    stu_ans[0][0] == reference_terms[0]\n",
    "), \"Task 3a: Your system_rankings tuple must have 'party' as the first term.\"\n",
    "assert set(stu_ans[1]) == set(\n",
    "    reference_terms\n",
    "), \"Task 3a: Your system_rankings tuple is not a permutation of the permitted terms.\"  # must be a permutation of the official term set\n",
    "\n",
    "# check spearmanr\n",
    "assert isinstance(\n",
    "    stu_ans[2], tuple\n",
    "), \"Task 3a: Your third element must be a tuple (of two floats). \"\n",
    "assert (\n",
    "    len(stu_ans[2]) == 2\n",
    "), \"Task 3a: Your spearman output tuple should contain two floats.\"\n",
    "assert isinstance(\n",
    "    stu_ans[2][0], (float, np.floating)\n",
    "), \"Task 3a: Your spearman corr should be a float. \"\n",
    "assert isinstance(\n",
    "    stu_ans[2][1], (float, np.floating)\n",
    "), \"Task 3a: Your spearman p-val should be a float. \"\n",
    "\n",
    "\n",
    "# Some hidden tests\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-710cb95aafc08d00",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='Topic4'></a>\n",
    "## Topic 4: - Topic Coherence (30 points total).\n",
    "\n",
    "One measure of topic model quality that is used e.g. to determine the optimal number of topics for a corpus is *topic coherence*. This is a measure of how semantically related the top terms in a topic model are. Topic models with low coherence tend to be filled with seemingly random words and hard to interpret, while high coherence usually indicates a clear semantic theme that's easily understood.\n",
    "\n",
    "With their ability to represent word semantics, word embeddings are an ideal tool for computing topic coherence. In part 1, you'll implement a simple topic coherence function. In part 2, you'll apply that function to NMF topic modeling to find a setting for the number of topics that gives maximally coherent topic models.\n",
    "\n",
    "We're going to use the same `text8_model` W2VTransformer object, which implements the word2vec embedding, that you loaded for the previous question.  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-94d0a243f11950b9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='t4a'></a>\n",
    "### Task 4a - Average semantic distance as a text coherence measure (15 points).\n",
    "Implement a function that takes a list of terms (strings) as input and returns a positive float indicating their semantic coherence. Here is the algorithm you should use:\n",
    "\n",
    "1. For each input term, compute its word2vec embedding vector. One problem you might encounter is that some terms may not exist in the word2vec model. You get a \"KeyError\" exception when trying to transform that \"out-of-vocabulary\" term. You should ignore these terms: one way to do this is by wrapping your embedding call with a try/except statement that catches the KeyError and just ignores that word, and continues processing.\n",
    "\n",
    "2. Once you have the list of embedding vectors for the input terms, compute their pairwise cosine similarity. If there are $n$ embedding vectors, this step will result in an $n x n$ matrix D.  If for some reason there are no input terms remaining (they are all out-of-vocabulary) just return 0.\n",
    "\n",
    "3. Obviously the most similar word to a term is itself, indicated by a \"1\" on the diagonal of $D$. But we don't want those: we only care about the pairwise distances to *other* terms, so to deal that case, set the diagonal to zero.\n",
    "\n",
    "4. Return the mean over all pairwise distances in D (with self-distances set to zero).  This is our simple coherence measure.\n",
    "\n",
    "Be sure to try it out on some samples. For example, here's what our reference implementation returns:\n",
    "\n",
    "`topical_coherence(['car', 'airplane', 'taxi', 'bus', 'vehicle', 'transport'])`\n",
    "\n",
    "0.46063321000999874\n",
    "\n",
    "`topical_coherence(['apple', 'banana', 'cherry', 'watermelon', 'lemon', 'orange'])`\n",
    "\n",
    "0.43306025200419956\n",
    "\n",
    "`topical_coherence(['possible', 'mean', 'volcano', 'feature', 'record', 'quickly'])`\n",
    "\n",
    "0.1150558124192887\n",
    "\n",
    "Your function should return the above measure of topic coherence for the following three lists, as a tuple of three corresponding floats:\n",
    "\n",
    "`['train', 'car', 'bicycle', 'bus', 'vehicle', 'transport']`\n",
    "\n",
    "`['scsi', 'drive', 'computer', 'storage', 'megabyte']`\n",
    "\n",
    "`['introduction', 'pickle', 'guard', 'red', 'valiant']`  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5008174\n",
      "0.115055785\n"
     ]
    }
   ],
   "source": [
    "#SAMPLE SETUP\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def semantic_coherence(terms):\n",
    "    # Load pre-trained word2vec model\n",
    "    model = text8_model.transform(terms) #KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    \n",
    "    # Compute word2vec embedding vectors for input terms\n",
    "    embeddings = []\n",
    "    for term in range(0, len(terms)):\n",
    "        try:\n",
    "            embedding = model[term]\n",
    "            embeddings.append(embedding)\n",
    "        except KeyError:\n",
    "            # Ignore out-of-vocabulary terms\n",
    "            continue\n",
    "    \n",
    "    # Handle case where all input terms are out-of-vocabulary\n",
    "    if len(embeddings) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Compute pairwise cosine similarity\n",
    "    D = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Set diagonal to zero\n",
    "    np.fill_diagonal(D, 0)\n",
    "    \n",
    "    # Compute mean over all pairwise distances\n",
    "    coherence = np.mean(D)\n",
    "    \n",
    "    return coherence\n",
    "a= ['train', 'car', 'bicycle', 'bus', 'vehicle', 'transport']\n",
    "print(semantic_coherence(a)) #0.5008174\n",
    "\n",
    "b =['possible', 'mean', 'volcano', 'feature', 'record', 'quickly']\n",
    "print(semantic_coherence(b)) #0.115055785"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['train', 'car', 'bicycle', 'bus', 'vehicle', 'transport'], ('train', 'bus', 'vehicle', 'car', 'transport', 'bicycle'))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def a4_routine(word_set):\n",
    "    wordvecs = text8_model.transform(word_set)   #makes nd.array \n",
    "    tw_embed = wordvecs[0]\n",
    "    analysis = []\n",
    "    for i in range(0, len(word_set)):\n",
    "        # print(\"for \",my_ranking[i], wordvecs[i])\n",
    "        similarity = 1 - cosine(tw_embed, wordvecs[i])\n",
    "        analysis.append((similarity,word_set[i]))\n",
    "    # Sort the list in decreasing order by the first element\n",
    "    analysis.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    # Extract a list of strings containing only the second element of each tuple\n",
    "    system_ranking = tuple([word for score, word in analysis])\n",
    "    result=(word_set, system_ranking)\n",
    "    return result\n",
    "print(a4_routine(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c98ec559f8390f02",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "a= ['train', 'car', 'bicycle', 'bus', 'vehicle', 'transport']\n",
    "b= ['scsi', 'drive', 'computer', 'storage', 'megabyte']\n",
    "c= ['introduction', 'pickle', 'guard', 'red', 'valiant']\n",
    "def getScore(terms):\n",
    "    # Load pre-trained word2vec model\n",
    "    model = text8_model.transform(terms) #KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    \n",
    "    # Compute word2vec embedding vectors for input terms\n",
    "    embeddings = []\n",
    "    for term in range(0, len(terms)):\n",
    "        try:\n",
    "            embedding = model[term]\n",
    "            embeddings.append(embedding)\n",
    "        except KeyError:\n",
    "            # Ignore out-of-vocabulary terms\n",
    "            continue\n",
    "    \n",
    "    # Handle case where all input terms are out-of-vocabulary\n",
    "    if len(embeddings) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Compute pairwise cosine similarity\n",
    "    D = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Set diagonal to zero\n",
    "    np.fill_diagonal(D, 0)\n",
    "    \n",
    "    # Compute mean over all pairwise distances\n",
    "    coherence = np.mean(D)\n",
    "    \n",
    "    return coherence\n",
    "\n",
    "def answer_coherence_a():\n",
    "    result =[]\n",
    "    total = [a,b,c]\n",
    "    for i in range(len(total)):\n",
    "        result.append(getScore(total[i]))\n",
    "\n",
    "    return tuple(result)\n",
    "# print(answer_coherence_a())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell to explore your solution\n",
    "# remember to comment the function call before submitting the notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "4a-graded",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 4a - AG tests\n",
      "Task 4a - your answer: (0.5008174, 0.44538254, 0.10494587)\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "task_id = \"4a\"\n",
    "print(f\"Task {task_id} - AG tests\")\n",
    "stu_ans = answer_coherence_a()\n",
    "\n",
    "print(f\"Task {task_id} - your answer: {stu_ans}\")\n",
    "\n",
    "assert isinstance(stu_ans, tuple), \"Task 4a: Your function should return a tuple. \"\n",
    "\n",
    "assert (\n",
    "    len(stu_ans) == 3\n",
    "), \"Task 4a: Your function should return a tuple of three elements. \"\n",
    "\n",
    "for i, item in enumerate(stu_ans):\n",
    "    assert isinstance(\n",
    "        item, (float, np.floating)\n",
    "    ), f\"Task 4a: Your answer at index {i} should be a float number. \"\n",
    "\n",
    "# Some hidden tests\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-218be1194a1f822d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='t4b'></a>\n",
    "### Task 4b - Applying semantic coherence to topic model selection (15 points).\n",
    "\n",
    "Now you'll use the semantic coherence measure you developed in Part 1 with topic models computed using Non-Negative Matrix Factorization.\n",
    "\n",
    "Implement a simple loop that trains an NMF topic model, for number of topics **from 2 to 10 inclusive**. At each iteration, compute your topic coherence measure on the **top 10** words for each topic. Then compute the *median* topic coherence over all these topic scores.\n",
    "\n",
    "Your function should return a list of 9 median coherence scores, corresponding to each choice of the number of topics to use with NMF.  Which choice gives the highest median semantic coherence?\n",
    "\n",
    "When creating the NMF object, use these parameter settings: `random_state=42, init=\"nndsvd\"`.  \n",
    "\n",
    "<a href='#toc'>TOC</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2840e7647d40fa8f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### Use the following code to prepare input to the NMF topic model.\n",
    "### It assumes you've loaded the 20newgroups variables at the beginning of this assignment\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer_NMF = TfidfVectorizer(\n",
    "    max_features=20000,\n",
    "    lowercase=True,\n",
    "    ngram_range=(1, 1),\n",
    "    min_df=2,\n",
    "    max_df=0.05,\n",
    "    # remove short, non-word-like terms\n",
    "    token_pattern=r\"\\b[a-z]{3,12}\\b\",\n",
    "    stop_words=\"english\",\n",
    ")\n",
    "\n",
    "tfidf_documents_NMF = tfidf_vectorizer_NMF.fit_transform(documents_train)\n",
    "\n",
    "feature_names_NMF = tfidf_vectorizer_NMF.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cea38c8b4154639b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def answer_coherence_b():\n",
    "    topic_index_max = 11\n",
    "    median_coherence_scores = []\n",
    "    for topic_index in range(2, topic_index_max):\n",
    "        nmf = NMF(n_components=topic_index, random_state=42, init=\"nndsvd\")\n",
    "        W = nmf.fit_transform(tfidf_documents_NMF)  # nmf.fit_transform(tfidf)\n",
    "        H = nmf.components_\n",
    "        \n",
    "        topic_coherence_scores = []\n",
    "        for topic_idx, topic in enumerate(H):\n",
    "            top_words_idx = topic.argsort()[:-11:-1]\n",
    "            top_words = [feature_names_NMF[i] for i in top_words_idx]\n",
    "            coherence_score = getScore(top_words)\n",
    "            topic_coherence_scores.append(coherence_score)\n",
    "        \n",
    "        median_coherence = np.median(topic_coherence_scores)\n",
    "        median_coherence_scores.append(median_coherence)  #, topic_index))\n",
    "    # sorted_list = sorted(median_coherence_scores, key=lambda x: x[0])\n",
    "    # top_value = sorted_list[0]\n",
    "    return median_coherence_scores #top_value[1]\n",
    "# print(answer_coherence_b())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "4b-graded",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 4b - AG tests\n",
      "Task 4b - your answer: [0.2929746, 0.23651, 0.24788213, 0.25925425, 0.35435927, 0.3755494, 0.3645056, 0.3755494, 0.36107093]\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "task_id = \"4b\"\n",
    "print(f\"Task {task_id} - AG tests\")\n",
    "stu_ans = answer_coherence_b()\n",
    "\n",
    "print(f\"Task {task_id} - your answer: {stu_ans}\")\n",
    "\n",
    "assert isinstance(stu_ans, list), \"Q4.2: Your function should return a list. \"\n",
    "\n",
    "assert (\n",
    "    len(stu_ans) == 9\n",
    "), \"Task 4b: Your function should return a list of nine elements (topic count 2 thru 10). \"\n",
    "\n",
    "for i, item in enumerate(stu_ans):\n",
    "    assert isinstance(\n",
    "        item, (float, np.floating)\n",
    "    ), f\"Task 4b: Your answer at index {i} should be a float number. \"\n",
    "\n",
    "\n",
    "# Some hidden tests\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d08a36621bd6358d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a href='#toc'>TOC</a>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python3.8",
   "language": "python",
   "name": "python3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
